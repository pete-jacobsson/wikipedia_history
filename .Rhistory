filter(decade > -500) %>%
acf(.resid)
model_2_pred_resid
model_2_pred_resid %>%
filter(decade > -500)
model_2_pred_resid %>%
filter(decade > -500) %>%
pull(.resid)
model_2_pred_resid %>%
filter(decade > -500) %>%
pull(.resid) %>%
acf()
model_2_pred_resid %>%
filter(decade > -500) %>%
pull(.resid) %>%
acf()
model_2_pred_resid %>%
ggplot(aes(x = decade)) +
geom_point(aes(y = .resid)) +
xlim(0, 600)#residuals on the log scale
model_2_pred_resid %>%
filter(decade > -500) %>%
pull(.resid) %>%
acf()
model_2_pred_resid %>%
ggplot(aes(x = decade)) +
geom_point(aes(y = .resid)) +
xlim(600, 1200)
model_2_pred_resid %>%
ggplot(aes(x = decade)) +
geom_point(aes(y = .resid)) +
xlim(600, 1200) +
ylim(-1, 1)
model_2_pred_resid %>%
ggplot(aes(x = decade)) +
geom_point(aes(y = .resid)) +
xlim(0, 600)
model_2_pred_resid %>%
ggplot(aes(x = decade)) +
geom_point(aes(y = .resid)) +
xlim(0, 600) +
ylim(-1, 1) +
geom_smooth()
model_2_pred_resid %>%
ggplot(aes(x = decade)) +
geom_point(aes(y = .resid)) +
xlim(0, 600) +
ylim(-1, 1)
model_2_pred_resid %>%
ggplot(aes(x = decade)) +
geom_point(aes(y = .resid)) +
xlim(0, 600) +
ylim(-1, 1) +
geom_smooth()
model_2_pred_resid %>%
ggplot(aes(x = decade, y = .resid)) +
geom_point() +
xlim(0, 600) +
ylim(-1, 1) +
geom_smooth()
model_2_pred_resid %>%
ggplot(aes(x = decade, y = .resid))) +
model_2_pred_resid %>%
ggplot(aes(x = decade, y = .resid)) +
geom_point() +
xlim(600, 1200) +
ylim(-1, 1) +
geom_smooth()
model_2_pred_resid %>%
ggplot(aes(x = decade, y = .resid)) +
geom_point() +
xlim(-500, 1200) +
ylim(-1, 1) +
geom_smooth()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(purrr)
library(broom)
events_df <- read_csv("events_df.csv")
events_by_decade <- events_df %>%
group_by(decade) %>%
summarize(
n_events  = n()
)
#load and log the data
events_df <- read_csv("events_df.csv")
events_by_decade <- events_df %>%
group_by(decade) %>%
summarize(
n_events  = n()
)
#fit the model residuals
model_1 <- events_by_decade %>%
filter(decade > -500 & decade < 1200) %>%
lm(log(n_events) ~ decade, data = .)
#Visualize model fit and the residuals
model_pred_resid <- augment(model_1, newdata = events_by_decade %>%
filter(decade < 1200))
# Determining the decades in question
model_pred_resid <- model_pred_resid %>%
mutate(greco_roman_events = if_else(.resid > 0.3, TRUE, FALSE))
model_pred_resid %>%
ggplot(aes(x = decade)) +
geom_point(aes(y = .resid, color = greco_roman_events)) +
xlim(-500, 50)# plot this to visualize the "capture" net.
model_pred_resid %>%
ggplot(aes(x = decade)) +
geom_point(aes(y = .resid, color = greco_roman_events)) +
xlim(-500, 1200)# plot this to visualize the "capture" net.
model_pred_resid %>%
ggplot(aes(x = decade)) +
geom_point(aes(y = .resid, color = greco_roman_events)) +
xlim(-500, 1200)# plot this to visualize the "capture" net.
events_by_decade %>%
ggplot(aes(
x = decade,
y = n_events
)) +
geom_point() +
scale_y_log10() +
theme_bw() +
xlab("Year (BC/AD)") +
ylab("Events per decade (log scale)") +
geom_vline(xintercept = -900, linetype = "dashed") +
geom_vline(xintercept = -500, linetype = "dashed") +
geom_text(x = -200, y = 1000, label = "Recording discontinuity \n at 500 BC") +
geom_text(x = -1300, y = 100, label = "Change in the rate of accumulation of historical data at 900 BC")
events_by_decade %>%
ggplot(aes(
x = decade,
y = n_events
)) +
geom_point() +
scale_y_log10() +
theme_bw() +
xlab("Year (BC/AD)") +
ylab("Events per decade (log scale)") +
# geom_vline(xintercept = -900, linetype = "dashed") +
#geom_vline(xintercept = -500, linetype = "dashed") +
geom_text(x = -200, y = 1000, label = "Recording discontinuity \n at 500 BC") +
geom_text(x = -1300, y = 100, label = "Change in the rate of accumulation of historical data at 900 BC")
model_pred_resid %>%
ggplot(aes(x = decade)) +
geom_line(aes(y = .fitted)) +
geom_point(aes(y = log(n_events))) #Fit on the log scale
model_pred_resid %>%
ggplot(aes(x = decade)) +
geom_line(aes(y = exp(.fitted))) +
geom_point(aes(y = n_events)) # Fit on the original scale
model_pred_resid %>%
ggplot(aes(x = decade)) +
geom_point(aes(y = .resid)) #residuals on the log scale
model_pred_resid %>%
mutate(exp_resid  = n_events - exp(.fitted)) %>%
ggplot(aes(x = decade)) +
geom_point(aes(y = exp_resid)) #re-do residuals from predict on the regression curve
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
variable_bc <- str_c(dates_bc, "_BC")
dates_bc <- seq(from = 1, to = 600)
dates_ad <- seq(from = 1, to = 2020)
variable_bc <- str_c(dates_bc, "_BC")
variable_ad <- str_c("AD_", dates_ad)
url_constant <- "https://en.wikipedia.org/wiki/"
dates_bc <- seq(from = 1, to = 600)
dates_ad <- seq(from = 1, to = 2020)
variable_bc <- str_c(dates_bc, "_BC")
variable_ad <- str_c("AD_", dates_ad)
wiki_decade_urls <- c(str_c(url_constant, variable_bc),
str_c(url_constant, variable_ad))
en_annual_events_df <- data.frame()
wiki_annual_urls <- c(str_c(url_constant, variable_bc),
str_c(url_constant, variable_ad))
url_constant <- "https://en.wikipedia.org/wiki/"
dates_bc <- seq(from = 1, to = 600)
dates_ad <- seq(from = 1, to = 2020)
variable_bc <- str_c(dates_bc, "_BC")
variable_ad <- str_c("AD_", dates_ad)
wiki_annual_urls <- c(str_c(url_constant, variable_bc),
str_c(url_constant, variable_ad))
en_annual_events_df <- data.frame()
wiki_decade_urls[1]
for (year in wiki_annual_urls[1:10]) {
year_scraped <- read_html(year)
year_events <- c()
events_scraped <- year_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
year_events <- c(year_events, year_scraped)
year_df <- data.frame(
wikipedia_urls = rep(year, length(year_events)),
event = year_events
)
Sys.sleep(4)
events_df <- bind_rows(en_annual_events_df, year_df)
}
year_scraped <- read_html(wiki_annual_urls[1])
year_events <- c()
events_scraped <- year_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
year_events <- c(year_events, year_scraped)
year_df <- data.frame(
wikipedia_urls = rep(year, length(year_events)),
event = year_events
)
year_df <- data.frame(
wikipedia_urls = rep(wiki_annual_urls[1], length(year_events)),
event = year_events
)
year_scraped <- read_html(wiki_annual_urls[1])
year_scraped <- read_html(wiki_annual_urls[1])
year_events <- c()
events_scraped <- year_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
year_events <- c(year_events, year_scraped)
year_scraped <- read_html(wiki_annual_urls[1])
events_scraped <- year_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
year_df <- data.frame(
wikipedia_urls = rep(wiki_annual_urls[1], length(events_scraped)),
event = events_scraped
)
url_constant <- "https://en.wikipedia.org/wiki/"
dates_bc <- seq(from = 1, to = 600)
dates_ad <- seq(from = 1, to = 2020)
variable_bc <- str_c(dates_bc, "_BC")
variable_ad <- str_c("AD_", dates_ad)
wiki_annual_urls <- c(str_c(url_constant, variable_bc),
str_c(url_constant, variable_ad))
en_annual_events_df <- data.frame()
wiki_decade_urls[1]
for (year in wiki_annual_urls[1:10]) {
year_scraped <- read_html(year)
events_scraped <- year_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
year_df <- data.frame(
wikipedia_urls = rep(year, length(events_scraped)),
event = events_scraped
)
Sys.sleep(4)
events_df <- bind_rows(en_annual_events_df, year_df)
}
url_constant <- "https://en.wikipedia.org/wiki/"
dates_bc <- seq(from = 1, to = 600)
dates_ad <- seq(from = 1, to = 2020)
variable_bc <- str_c(dates_bc, "_BC")
variable_ad <- str_c("AD_", dates_ad)
wiki_annual_urls <- c(str_c(url_constant, variable_bc),
str_c(url_constant, variable_ad))
en_annual_events_df <- data.frame()
wiki_decade_urls[1]
for (year in wiki_annual_urls[1:3]) {
year_scraped <- read_html(year)
events_scraped <- year_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
year_df <- data.frame(
wikipedia_urls = rep(year, length(events_scraped)),
event = events_scraped
)
Sys.sleep(4)
en_annual_events_df <- bind_rows(en_annual_events_df, year_df)
}
View(en_annual_events_df)
url_constant <- "https://en.wikipedia.org/wiki/"
dates_bc <- seq(from = 1, to = 600)
dates_ad <- seq(from = 1, to = 2020)
variable_bc <- str_c(dates_bc, "_BC")
variable_ad <- str_c("AD_", dates_ad)
wiki_annual_urls <- c(str_c(url_constant, variable_bc),
str_c(url_constant, variable_ad))
en_annual_events_df <- data.frame()
wiki_decade_urls[1]
last_url_run <- 0
for (year in wiki_annual_urls) {
last_url_run <- last_url_run + 1
year_scraped <- read_html(year)
events_scraped <- year_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
year_df <- data.frame(
wikipedia_urls = rep(year, length(events_scraped)),
event = events_scraped
)
Sys.sleep(4)
en_annual_events_df <- bind_rows(en_annual_events_df, year_df)
}
write_csv(en_annual_events_df, "en_annual_events.csv")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(purrr)
library(broom)
#load and log the data
events_df <- read_csv("events_df.csv")
events_df_cleaning <- en_annual_events_df %>%
mutate(is_bc = str_detect(wikipedia_urls, "BC"),
year = str_extract(wikipedia_urls, ("\\d+")) %>%
parse_number()
) %>%
mutate(decade = if_else(is_bc == TRUE, decade * -1, decade))
events_df_cleaning <- en_annual_events_df %>%
mutate(is_bc = str_detect(wikipedia_urls, "BC"),
year = str_extract(wikipedia_urls, ("\\d+")) %>%
parse_number()
) %>%
mutate(decade = if_else(is_bc == TRUE, year * -1, year))
head(events_df_cleaning)
events_df_cleaning <- en_annual_events_df %>%
mutate(is_bc = str_detect(wikipedia_urls, "BC"),
year = str_extract(wikipedia_urls, ("\\d+")) %>%
parse_number()
) %>%
mutate(year = if_else(is_bc == TRUE, year * -1, year))
head(events_df_cleaning)
##OK this will get cut: getting ahead, but fun!
events_df_grouped <- events_df_cleaning %>%
group_by(year) %>%
summarise(
n_events = n()
)
events_df_grouped %>%
ggplot(aes(x = year, y = n_events)) +
geom_point()
events_df_grouped %>%
ggplot(aes(x = year, y = n_events)) +
geom_point() +
scale_y_log10()
events_df_grouped %>%
ggplot(aes(x = year, y = n_events)) +
geom_point()
events_df_grouped %>%
ggplot(aes(x = year, y = n_events)) +
geom_point()
events_df_grouped %>%
ggplot(aes(x = year, y = n_events)) +
geom_point() +
scale_y_log10()
events_df_grouped %>%
ggplot(aes(x = year, y = n_events)) +
geom_point() +
scale_y_log10() +
xlim(c(1500, 2000))
events_df_grouped %>%
ggplot(aes(x = year, y = n_events)) +
geom_point() +
xlim(c(1600, 2020))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
url_constant <- "https://fr.wikipedia.org/wiki/"
dates_bc <- seq(from = 0, to = 2070, by = 10)
dates_bc <- seq(from = 0, to = 2070, by = 10)
dates_ad <- seq(from = 0, to = 2010, by = 10)
variable_bc <- str_c("Années_", dates_bc, "_av._J.-C.")
variable_ad <- str_c("Années_", dates_ad)
wiki_decade_urls <- c(str_c(url_constant, variable_bc),
str_c(url_constant, variable_ad))
fr_decade_events_df <- data.frame()
wiki_decade_urls[1]
for (decade in wiki_decade_urls[1:3]) {
decade_scraped <- read_html(decade)
decade_events <- c()
events_scraped <- decade_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
decade_events <- c(decade_events, events_scraped)
decade_df <- data.frame(
wikipedia_urls = rep(decade, length(decade_events)),
event = decade_events
)
Sys.sleep(4)
fr_decade_events_df <- bind_rows(fr_decade_events_df, decade_df)
}
View(fr_decade_events_df)
url_constant <- "https://fr.wikipedia.org/wiki/"
dates_bc <- seq(from = 1, to = 500)
dates_ad <- seq(from = 1, to = 1854)
variable_bc <- str_c(dates_bc, "_av._J.-C.")
variable_ad <- str_c(dates_ad)
wiki_annual_urls <- c(str_c(url_constant, variable_bc),
str_c(url_constant, variable_ad))
fr_annual_events_df <- data.frame()
wiki_decade_urls[1]
for (year in wiki_annual_urls[1:3]) {
last_url_run <- last_url_run + 1
year_scraped <- read_html(year)
events_scraped <- year_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
year_df <- data.frame(
wikipedia_urls = rep(year, length(events_scraped)),
event = events_scraped
)
Sys.sleep(4)
fr_annual_events_df <- bind_rows(en_annual_events_df, year_df)
}
last_url_run <- 0
for (year in wiki_annual_urls[1:3]) {
last_url_run <- last_url_run + 1
year_scraped <- read_html(year)
events_scraped <- year_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
year_df <- data.frame(
wikipedia_urls = rep(year, length(events_scraped)),
event = events_scraped
)
Sys.sleep(4)
fr_annual_events_df <- bind_rows(en_annual_events_df, year_df)
}
for (year in wiki_annual_urls[1:3]) {
last_url_run <- last_url_run + 1
year_scraped <- read_html(year)
events_scraped <- year_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
year_df <- data.frame(
wikipedia_urls = rep(year, length(events_scraped)),
event = events_scraped
)
Sys.sleep(4)
fr_annual_events_df <- bind_rows(fr_annual_events_df, year_df)
}
View(fr_annual_events_df)
View(fr_annual_events_df)
for (year in wiki_annual_urls[499:501]) {
last_url_run <- last_url_run + 1
year_scraped <- read_html(year)
events_scraped <- year_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
year_df <- data.frame(
wikipedia_urls = rep(year, length(events_scraped)),
event = events_scraped
)
Sys.sleep(4)
fr_annual_events_df <- bind_rows(fr_annual_events_df, year_df)
}
View(fr_annual_events_df)
fr_annual_events_df <- data.frame()
for (year in wiki_annual_urls[1499:1501]) {
last_url_run <- last_url_run + 1
year_scraped <- read_html(year)
events_scraped <- year_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
year_df <- data.frame(
wikipedia_urls = rep(year, length(events_scraped)),
event = events_scraped
)
Sys.sleep(4)
fr_annual_events_df <- bind_rows(fr_annual_events_df, year_df)
}
View(fr_annual_events_df)
url_constant <- "https://ru.wikipedia.org/wiki/"
dates_bc <- seq(from = 0, to = 1210, by = 10)
dates_ad <- seq(from = 0, to = 2010, by = 10)
variable_bc <- str_c(dates_bc, "-е_годы_до_н._э.")
variable_ad <- str_c(dates_ad, "-е_годы")
wiki_decade_urls <- c(str_c(url_constant, variable_bc),
str_c(url_constant, variable_ad))
ru_decade_events_df <- data.frame()
wiki_decade_urls[1]
for (decade in wiki_decade_urls[1:3]) {
decade_scraped <- read_html(decade)
decade_events <- c()
events_scraped <- decade_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
decade_events <- c(decade_events, events_scraped)
decade_df <- data.frame(
wikipedia_urls = rep(decade, length(decade_events)),
event = decade_events
)
Sys.sleep(4)
ru_decade_events_df <- bind_rows(ru_decade_events_df, decade_df)
}
View(ru_decade_events_df)
url_constant <- "https://ru.wikipedia.org/wiki/"
dates_bc <- seq(from = 1, to = 781)
dates_ad <- seq(from = 1, to = 2020)
variable_bc <- str_c(dates_bc, "_год_до_н._э.")
variable_ad <- str_c(dates_ad, "_год")
wiki_annual_urls <- c(str_c(url_constant, variable_bc),
str_c(url_constant, variable_ad))
ru_annual_events_df <- data.frame()
wiki_decade_urls[1]
wiki_annual_urls[1]
last_url_run <- 0
for (year in wiki_annual_urls[1:3]) {
last_url_run <- last_url_run + 1
year_scraped <- read_html(year)
events_scraped <- year_scraped %>%
html_nodes(".mw-parser-output > ul li") %>%
html_text()
year_df <- data.frame(
wikipedia_urls = rep(year, length(events_scraped)),
event = events_scraped
)
Sys.sleep(4)
ru_annual_events_df <- bind_rows(ru_annual_events_df, year_df)
}
View(ru_annual_events_df)
